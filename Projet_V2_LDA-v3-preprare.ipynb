{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2.75em;color:blue; font-style:bold\"><br>\n",
    "Projet 6 :<br><br>\n",
    "Catégorisez automatiquement des questions<br><br>\n",
    "</p><br>\n",
    "\n",
    "* **Data Source / Data Source :** \n",
    "    - Data :\n",
    "        - Outils d'extraction de data : https://data.stackexchange.com/stackoverflow\n",
    "        - Script : https://github.com/EricJacquesPro/TextCategorization/blob/master/SQL/ExtractionDonnees.sql\n",
    "    - Source Python : https://github.com/EricJacquesPro/TextCategorization\n",
    "* **Description / Description : ** Projet 6 de la formation Data Scientist Proposée par OpenClassroom\n",
    "    - Analyse LDA\n",
    "    - Entrainement LDA\n",
    "* **Auteur / Author : ** Eric JACQUES \n",
    "* **Date : ** 18/07/2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.insert(0, './Python/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python class for the data engineering (reading, cleaning, training...)\n",
    "localised in ./Python\n",
    "'''\n",
    "from tagText import TagText "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagText = TagText()\n",
    "print(tagText.urlDirectory)#folder for cvs file generated by SQL query (cf. I) \n",
    "print(tagText.fileName)#name of the cvs file generated by SQL query (cf. I)\n",
    "\n",
    "tagText.nombre_post_entree = 50000\n",
    "tagText.precision = 50000\n",
    "\n",
    "data_question = tagText.read_source()\n",
    "data_question.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = data_question['body'].apply(tagText.preprocessing)\n",
    "print(data_preprocessed[1:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "III.2 - Train/Test</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sampling dataset\n",
    "vectorizer_X = TfidfVectorizer(\n",
    "            max_df=0.95,\n",
    "            min_df=2,\n",
    "            max_features=50000,\n",
    "            stop_words='english'\n",
    "                              )\n",
    "\n",
    "#Y\n",
    "y_all = [\n",
    "    item[:-1].split(',')#-1 car il y a un ',' à la fin de la ligne\n",
    "    for item in data_question['SelectedTags']\n",
    "]\n",
    "\n",
    "#print(y_train_tag)\n",
    "lb = tagText.MultiLabelBinarizer()\n",
    "Y_all = lb.fit_transform(y_all)\n",
    "\n",
    "\n",
    "# 80/20 split\n",
    "X_lda_train, X_lda_test, y_lda_train, y_lda_test = train_test_split(\n",
    "    data_preprocessed, y_all, test_size=0.2,train_size=0.8, random_state=0)\n",
    "y_lda_train = lb.transform(y_lda_train)\n",
    "y_lda_test = lb.transform(y_lda_test)\n",
    "\n",
    "# TF-IDF matrices\n",
    "X_tfidf_train = vectorizer_X.fit_transform(X_lda_train)\n",
    "X_tfidf_test = vectorizer_X.transform(X_lda_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_lda_test)\n",
    "print(Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_lda_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_tfidf_train)\n",
    "print(X_tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = vectorizer_X.idf_\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=dict(zip(vectorizer_X.get_feature_names(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "# print(l)\n",
    "print(l[0],l[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(dd['like'])\n",
    "print(dd['consolewritelinesecond'])  # like is most common and forecast is least common among the news headlines.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=42,max_iter=1) \n",
    "lda_top=lda_model.fit_transform(X_tfidf_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composition of doc 0 for eg\n",
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lda_top)\n",
    "print(lda_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=2,\n",
    "        random_state=0\n",
    ")\n",
    "#lda_top=lda_model.fit_transform(X_tfidf_train)\n",
    "clf.fit(lda_top, y_lda_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = lb.classes_\n",
    "\n",
    "no_top_words = 5\n",
    "text = \"git is the best source code\"\n",
    "text = tagText.preprocessing(text)\n",
    "text = [text]\n",
    "mytext = vectorizer_X.transform(text)\n",
    "text_projection = lda_model.transform(mytext)\n",
    "'''\n",
    "print (text_projection)\n",
    "print (text_projection.shape)\n",
    "'''\n",
    "predicted = clf.predict_proba(text_projection)\n",
    "'''\n",
    "print (predicted)\n",
    "print (len(predicted))\n",
    "'''\n",
    "tempTag = [(1-item[0][0]) for item in predicted]\n",
    "list_id = [[i, x] for i, x in enumerate(tempTag) if x > 0.0050]\n",
    "'''\n",
    "tempTag = predicted[0]\n",
    "list_id = [[i, x] for i, x in enumerate(tempTag) ]#if x > 0.0050]\n",
    "'''\n",
    "'''\n",
    "print (predicted)\n",
    "print (predicted.shape)\n",
    "'''\n",
    "list_id_sorted = sorted(list_id, reverse=True, key=lambda x: x[1])\n",
    "#print (list_id_sorted)\n",
    "\n",
    "list_id_sorted_suggested = [x[0] for i, x in enumerate(list_id_sorted[:-no_top_words - 1:-1])]\n",
    "#print(list_id_sorted_suggested)\n",
    "\n",
    "print (str([classes[id] for id in list_id_sorted_suggested]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(x_test, y_true, clf, lb, mode_supervise_with_lda = False, lda_model = None):\n",
    "    nb_tag_1 = 0.0\n",
    "    nb_tag_5 = 0.0\n",
    "    classes = lb.classes_\n",
    "    print(x_test.shape[0])\n",
    "    no_top_words = 5\n",
    "    for i in range(x_test.shape[0]):\n",
    "        #for i in range(x_test.shape[0]):\n",
    "        text_projection = x_test\n",
    "        if(mode_supervise_with_lda):\n",
    "            text_projection = lda_model.transform(x_test[i])\n",
    "        '''\n",
    "        print (text_projection)\n",
    "        print (text_projection.shape)\n",
    "        '''\n",
    "        predicted = clf.predict_proba(text_projection)\n",
    "        '''\n",
    "        print (predicted)\n",
    "        print (len(predicted))\n",
    "        '''\n",
    "        tempTag = [(1-item[0][0]) for item in predicted]\n",
    "        list_id = [[i, x] for i, x in enumerate(tempTag) if x > 0.0050]\n",
    "        '''\n",
    "        tempTag = predicted[0]\n",
    "        list_id = [[i, x] for i, x in enumerate(tempTag) ]#if x > 0.0050]\n",
    "        '''\n",
    "        '''\n",
    "        print (predicted)\n",
    "        print (predicted.shape)\n",
    "        '''\n",
    "        list_id_sorted = sorted(list_id, reverse=True, key=lambda x: x[1])\n",
    "        #print (list_id_sorted)\n",
    "\n",
    "        list_id_sorted_suggested = [x[0] for i, x in enumerate(list_id_sorted[:-no_top_words - 1:-1])]\n",
    "        #print(list_id_sorted_suggested)\n",
    "        prediction = [classes[id] for id in list_id_sorted_suggested]\n",
    "        #print (str(prediction))\n",
    "        \n",
    "        \n",
    "        l_y = [[i, x] for i, x in enumerate(y_true[i]) if x > 0]\n",
    "        l_y_tagged = [x[0] for i, x in enumerate(l_y[:-no_top_words - 1:-1])]\n",
    "        l_y_tags = [classes[id] for id in l_y_tagged]\n",
    "        #print (l_y_tags)\n",
    "        \n",
    "        check_1 = False\n",
    "        check_1 = any(item in prediction for item in l_y_tags)\n",
    "\n",
    "        if check_1 is True:\n",
    "            nb_tag_1 = nb_tag_1 + 1\n",
    "        \"\"\"    print(\"The list {} contains some elements of the list {}\".format(prediction, l_y_tags))    \n",
    "        else :\n",
    "            print(\"No, List1 doesn't have any elements of the List2.\")\n",
    "        \"\"\"\n",
    "\n",
    "        check_5 = False\n",
    "        check_5 = all(item in prediction for item in l_y_tags)\n",
    "        if check_5 is True:\n",
    "            nb_tag_5 = nb_tag_5 + 1\n",
    "        \"\"\"    print(\"The list {} contains all elements of the list {}\".format(prediction, l_y_tags))    \n",
    "        else :\n",
    "            print(\"No, List1 doesn't have any elements of the List2.\")\n",
    "        \"\"\"\n",
    "        #str([tag for tag in y_true[i]]if tag ==1)\n",
    "    return nb_tag_1, (100.0 * nb_tag_1 / float(x_test.shape[0])), nb_tag_5, (100.0 * nb_tag_5 / float(x_test.shape[0]))\n",
    "\n",
    "scoring(X_tfidf_test, y_lda_test, clf, lb, True, lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "IV.2-a - Data - Tag generator - Unsupervised - LDA</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_prepare_tag(tagText, data_preprocessed, data_tag, no_tropics=32):\n",
    "        '''\n",
    "        prepare lda, topic ad tf vectorizer from data preprocessed\n",
    "        '''\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        tagText.n_topic = no_tropics\n",
    "        documents = data_preprocessed[0:tagText.precision]#.unique()\n",
    "        lda_tf, lda_tf_vectorizer = tagText.lda_init(documents)\n",
    "        \n",
    "        lda, score, perplexity = tagText.lda_train(lda_tf, no_tropics)\n",
    "        \n",
    "        print(\"Log Likelihood: \", score)\n",
    "        print(\"Perplexity: \", perplexity)\n",
    "\n",
    "        # See model parameters\n",
    "        print(lda.get_params())\n",
    "\n",
    "        lda_topicnames = [\"Topic\" + str(i) for i in range(lda.n_topics)]\n",
    "        # Topic-Keyword Matrix\n",
    "        lda_df_topic_keywords = tagText.pd.DataFrame(lda.components_)\n",
    "        \n",
    "        # Assign Column and Index\n",
    "        lda_df_topic_keywords.columns = lda_tf_vectorizer.get_feature_names()\n",
    "        lda_df_topic_keywords.index = lda_topicnames\n",
    "        \n",
    "        #tag\n",
    "        topic_word = lda.topic_word_\n",
    "        print(\"type(topic_word): {}\".format(type(topic_word)))\n",
    "        print(\"shape: {}\".format(topic_word.shape))\n",
    "        \n",
    "        clf = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=2,\n",
    "                random_state=0\n",
    "        )\n",
    "        clf.fit(lda, y_lda_train)\n",
    "        \n",
    "        return lda, lda_df_topic_keywords, lda_tf_vectorizer, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tagText, df_topic_keywords, tf_vectorizer, clf = lda_prepare_tag(tagText, X_tfidf_train, y_lda_train, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_jaccard(y_true,y_pred):\n",
    "\n",
    "    ''' It calculates Jaccard similarity coefficient score for each instance,and\n",
    "    it finds their averange in percentage\n",
    "    Parameters:\n",
    "    y_true: truth labels\n",
    "    y_pred: predicted labels\n",
    "    '''\n",
    "    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n",
    "    \n",
    "    return jacard.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most important words for each topic\n",
    "vocab = vectorizer_X.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lda_tagText.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
